{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhk/miniconda3/envs/icta2024/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nhk/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3975.74 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 5091.16 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 2500.27 examples/s]\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 45,000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 14,060\n",
      "  Number of trainable parameters = 8,912,896\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnguyenkhanh87\u001b[0m (\u001b[33mdectnu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nhk/phi3/wandb/run-20240712_231409-muhn1van</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dectnu/CHiLPhi3m4/runs/muhn1van' target=\"_blank\">2024-07-12-23-14</a></strong> to <a href='https://wandb.ai/dectnu/CHiLPhi3m4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dectnu/CHiLPhi3m4' target=\"_blank\">https://wandb.ai/dectnu/CHiLPhi3m4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dectnu/CHiLPhi3m4/runs/muhn1van' target=\"_blank\">https://wandb.ai/dectnu/CHiLPhi3m4/runs/muhn1van</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1632' max='14060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1632/14060 6:56:44 < 52:57:25, 0.07 it/s, Epoch 0.58/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.752500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.525600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.513000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 2\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "import os, gc, time, evaluate\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "clear_memory()\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME \"] = \"CHiLPhi3m4\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"CHiLPhi3m4\"\n",
    "os.environ[\"HF_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "local_model_dir = \"./CHiLPhi3m4\"\n",
    "\n",
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model from Hugging Face hub that you want to fine-tune.\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "new_model = \"CHiLPhi3m4\"\n",
    "hf_model_repo=\"nguyenkhanh87/\"+new_model\n",
    "dataset_name = \"lex_glue\"\n",
    "\n",
    "# Load Model on GPU \n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Bits and Bytes configuration for the model\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_double_quant = True\n",
    "\n",
    "# LoRA configuration for the model\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "set_seed(1234)\n",
    "\n",
    "isSetupParams = False\n",
    "if isSetupParams:\n",
    "    dataset = load_dataset(\"lex_glue\", \"case_hold\")\n",
    "    patience = 1\n",
    "    train_batch_size = 1\n",
    "    eval_batch_size = 1\n",
    "    evalsteps = 1\n",
    "    logsteps = 1\n",
    "    num_train_epochs = 1\n",
    "    save_step = 1\n",
    "else:\n",
    "    dataset = load_dataset(\"lex_glue\", \"case_hold\")\n",
    "    patience = 10\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 2\n",
    "    evalsteps = 500\n",
    "    logsteps = 500\n",
    "    num_train_epochs = 5\n",
    "    save_step = 2000\n",
    "\n",
    "tokenizer_id = model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(row['endings'])])\n",
    "    user = {\n",
    "        \"content\": f\"Choose best option to fill <HOLDING> base on this context, answer only with the number of option(for example: 0):\\n{row['context']}\\n Options to choose:\\n {answers}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['label']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    messages.append(assistant)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def create_message_column_val(row):\n",
    "    messages = []\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(row['endings'])])\n",
    "    user = {\n",
    "        \"content\": f\"Choose best option to fill <HOLDING> base on this context, answer only with the number of option(for example: 0):\\n{row['context']}\\n Options to choose:\\n {answers}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "# if isSetupParams is True, take only 20 example\n",
    "if isSetupParams:\n",
    "    dataset['train'] = dataset['train'].select(list(range(20)))\n",
    "    dataset['validation'] = dataset['validation'].select(list(range(20)))\n",
    "\n",
    "dataset_chatml = dataset['train'].map(create_message_column)\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "dataset_val = dataset['validation'].take(20).map(create_message_column_val)\n",
    "dataset_val = dataset_val.map(format_dataset_chatml)\n",
    "\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_double_quant = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_double_quant,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n",
    "          attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "#model2 = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map).base_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "# f1 score compute metric for the model\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    logger.info(f'Predictions: {predictions}')\n",
    "    logger.info(f'Labels: {labels}')\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"f1\": f1_score(y_true=labels, y_pred=predictions)}\n",
    "\n",
    "class CustomEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=5, threshold=0.0):\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.patience_counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.epoch > 1: # at least 1 epoch\n",
    "            if state.log_history:\n",
    "                current_loss = state.log_history[0].get('loss')\n",
    "                if current_loss is not None:\n",
    "                    if current_loss < self.best_loss - self.threshold:\n",
    "                        self.best_loss = current_loss\n",
    "                        self.patience_counter = 0\n",
    "                    else:\n",
    "                        self.patience_counter += 1\n",
    "\n",
    "                    if self.patience_counter >= self.patience:\n",
    "                        control.should_training_stop = True\n",
    "                        print(\"Early stopping triggered!\")\n",
    "        return control\n",
    "\n",
    "early_stopping = CustomEarlyStoppingCallback(patience=patience)\n",
    "\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def _save_checkpoint(self, model, trial=None, metrics=None):\n",
    "        # Copy from Trainer but remove 2 lines to avoid eval_ prefix\n",
    "        checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "        if self.hp_search_backend is None and trial is None:\n",
    "            self.store_flos()\n",
    "        run_dir = self._get_output_dir(trial=trial)\n",
    "        output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "        self.save_model(output_dir, _internal_call=True)\n",
    "        if not self.args.save_only_model:\n",
    "            self._save_optimizer_and_scheduler(output_dir)\n",
    "            self._save_rng_state(output_dir)\n",
    "        if metrics is not None and self.args.metric_for_best_model is not None:\n",
    "            metric_to_check = self.args.metric_for_best_model\n",
    "            print(metrics) # print metrics to check what is available\n",
    "            if metric_to_check in metrics:\n",
    "                metric_value = metrics[metric_to_check]\n",
    "                operator = np.greater if self.args.greater_is_better else np.less\n",
    "                if (\n",
    "                    self.state.best_metric is None\n",
    "                    or self.state.best_model_checkpoint is None\n",
    "                    or operator(metric_value, self.state.best_metric)\n",
    "                ):\n",
    "                    self.state.best_metric = metric_value\n",
    "                    self.state.best_model_checkpoint = output_dir\n",
    "        if self.args.should_save:\n",
    "            self.state.stateful_callbacks[\"TrainerControl\"] = self.control.state()\n",
    "            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n",
    "        if self.args.push_to_hub:\n",
    "            self._push_from_checkpoint(output_dir)\n",
    "        if self.args.should_save:\n",
    "            self._rotate_checkpoints(use_mtime=False, output_dir=run_dir)\n",
    "\n",
    "run_name = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=local_model_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    log_level=\"debug\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    eval_steps=evalsteps,\n",
    "    logging_steps=logsteps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=None,\n",
    "    seed=342,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    run_name=run_name,\n",
    "    save_steps=save_step,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    " \n",
    "trainer = CustomSFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_chatml,\n",
    "    eval_dataset=dataset_val,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(local_model_dir)\n",
    "\n",
    "# trainer.push_to_hub(hf_model_repo)\n",
    "\n",
    "tokenizer.save_pretrained(local_model_dir)\n",
    "model.save_pretrained(local_model_dir)\n",
    "\n",
    "trainer.push_to_hub(hf_model_repo)\n",
    "tokenizer.push_to_hub(hf_model_repo)\n",
    "model.push_to_hub(hf_model_repo)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
