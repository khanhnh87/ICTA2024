{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhk/miniconda3/envs/icta2024/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/nhk/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n",
      "Map: 100%|██████████| 45000/45000 [00:06<00:00, 7341.06 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2922.40 examples/s]\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 45,000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 14,060\n",
      "  Number of trainable parameters = 8,912,896\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnguyenkhanh87\u001b[0m (\u001b[33mdectnu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nhk/phi3/wandb/run-20240725_021258-i8ncggup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dectnu/CH-Phi3m4k/runs/i8ncggup' target=\"_blank\">2024-07-25-02-12</a></strong> to <a href='https://wandb.ai/dectnu/CH-Phi3m4k' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dectnu/CH-Phi3m4k' target=\"_blank\">https://wandb.ai/dectnu/CH-Phi3m4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dectnu/CH-Phi3m4k/runs/i8ncggup' target=\"_blank\">https://wandb.ai/dectnu/CH-Phi3m4k/runs/i8ncggup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='14060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3600/14060 31:50:41 < 92:34:43, 0.03 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.005300</td>\n",
       "      <td>1.892748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.648100</td>\n",
       "      <td>1.606505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.565300</td>\n",
       "      <td>1.586402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.551600</td>\n",
       "      <td>1.576210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.537000</td>\n",
       "      <td>1.568918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.531800</td>\n",
       "      <td>1.563681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.524900</td>\n",
       "      <td>1.559466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.520800</td>\n",
       "      <td>1.554966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.523400</td>\n",
       "      <td>1.551101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.509900</td>\n",
       "      <td>1.548010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.514400</td>\n",
       "      <td>1.544766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.513900</td>\n",
       "      <td>1.542385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.501300</td>\n",
       "      <td>1.539515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.502000</td>\n",
       "      <td>1.537650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.497900</td>\n",
       "      <td>1.535807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.491000</td>\n",
       "      <td>1.533795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.502500</td>\n",
       "      <td>1.531811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.493300</td>\n",
       "      <td>1.530445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-200\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-400\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-600\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-800\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-800/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-1000\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-1200\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-1200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-1400\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-1400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-1600\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-1600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-1800\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-1800/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-2000\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-2200\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-2200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-2400\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-2400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-2600\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-2600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-2800\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-2800/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-3000\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-3200\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-3200/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-3200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-3400\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-3400/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-3400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval f1: 0.772\n",
      "Early stopping triggered with best f1 is 0.778!\n",
      "Total steps: 3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./CH-Phi3m4k/checkpoint-3600\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/checkpoint-3600/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./CH-Phi3m4k\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/special_tokens_map.json\n",
      "tokenizer config file saved in ./CH-Phi3m4k/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/special_tokens_map.json\n",
      "Configuration saved in ./CH-Phi3m4k/config.json\n",
      "Configuration saved in ./CH-Phi3m4k/generation_config.json\n",
      "Model weights saved in ./CH-Phi3m4k/model.safetensors\n",
      "Saving model checkpoint to ./CH-Phi3m4k\n",
      "loading configuration file config.json from cache at /home/nhk/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./CH-Phi3m4k/tokenizer_config.json\n",
      "Special tokens file saved in ./CH-Phi3m4k/special_tokens_map.json\n",
      "model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "training_args.bin: 100%|██████████| 5.05k/5.05k [00:00<00:00, 14.1kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:02<00:00, 230kB/s]MB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 17.8M/17.8M [00:04<00:00, 3.89MB/s]\n",
      "model.safetensors:   2%|▏         | 39.7M/2.28G [00:04<03:41, 10.1MB/s]\n",
      "\n",
      "model.safetensors: 100%|██████████| 2.28G/2.28G [03:05<00:00, 12.3MB/s]\n",
      "\n",
      "\n",
      "Upload 4 LFS files: 100%|██████████| 4/4 [03:06<00:00, 46.56s/it] \n",
      "tokenizer config file saved in CH-Phi3m4k/tokenizer_config.json\n",
      "Special tokens file saved in CH-Phi3m4k/special_tokens_map.json\n",
      "Uploading the following files to nguyenkhanh87/CH-Phi3m4k: tokenizer.model,README.md,tokenizer.json,special_tokens_map.json,tokenizer_config.json,added_tokens.json\n",
      "Configuration saved in CH-Phi3m4k/config.json\n",
      "Configuration saved in CH-Phi3m4k/generation_config.json\n",
      "Model weights saved in CH-Phi3m4k/model.safetensors\n",
      "Uploading the following files to nguyenkhanh87/CH-Phi3m4k: README.md,model.safetensors,generation_config.json,config.json\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "import torch, re\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "import os, gc, time, evaluate\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "clear_memory()\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME \"] = \"CH-Phi3m4k\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"CH-Phi3m4k\"\n",
    "os.environ[\"HF_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "local_model_dir = \"./CH-Phi3m4k\"\n",
    "\n",
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model from Hugging Face hub that you want to fine-tune.\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "new_model = \"CH-Phi3m4k\"\n",
    "hf_model_repo=\"nguyenkhanh87/\"+new_model\n",
    "dataset_name = \"casehold/casehold\"\n",
    "\n",
    "# Load Model on GPU \n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Bits and Bytes configuration for the model\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_double_quant = True\n",
    "\n",
    "# LoRA configuration for the model\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "set_seed(1234)\n",
    "\n",
    "isSetupParams = False\n",
    "if isSetupParams:\n",
    "    dataset = load_dataset(\"lex_glue\", \"case_hold\")\n",
    "    patience = 1\n",
    "    train_batch_size = 1\n",
    "    eval_batch_size = 1\n",
    "    evalsteps = 20\n",
    "    logsteps = 1\n",
    "    num_train_epochs = 10\n",
    "    save_step = 20\n",
    "else:\n",
    "    dataset = load_dataset(\"lex_glue\", \"case_hold\")\n",
    "    patience = 5\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 1\n",
    "    evalsteps = 200\n",
    "    logsteps = 200\n",
    "    num_train_epochs = 10\n",
    "    save_step = 200\n",
    "\n",
    "tokenizer_id = model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(row['endings'])])\n",
    "    user = {\n",
    "        \"content\": f\"Choose best option to fill <HOLDING> base on this context, answer only with the number of option(for example: 0):\\n{row['context']}\\n Options to choose:\\n {answers}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['label']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    messages.append(assistant)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def create_message_column_val(row):\n",
    "    messages = []\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(row['endings'])])\n",
    "    user = {\n",
    "        \"content\": f\"Choose best option to fill <HOLDING> base on this context, answer only with the number of option(for example: 0):\\n{row['context']}\\n Options to choose:\\n {answers}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "def format_func(example):\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(example['endings'])])\n",
    "    text = f\"<|user|>\\n Choose the best option to fill <HOLDING> based on the context below, answer only with the number of option (for example: 0):\\n{example['context']}\\nOptions to choose:\\n{answers}<|end|>\\n<|assistant|>\\n{example['label']}<|end|>\"\n",
    "    return {\"text\": text, \"labels\": example[\"label\"], \"endings\": example[\"endings\"], \"context\":  example[\"context\"]}\n",
    "\n",
    "# if isSetupParams is True, take only 20 example\n",
    "if isSetupParams:\n",
    "    dataset['train'] = dataset['train'].select(list(range(200)))\n",
    "    dataset['validation'] = dataset['validation'].select(list(range(200)))\n",
    "else:\n",
    "    dataset['validation'] = dataset['validation'].select(list(range(500)))\n",
    "\n",
    "dataset_chatml = dataset['train'].map(format_func)\n",
    "# dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "dataset_val = dataset['validation'].map(format_func)\n",
    "# dataset_val = dataset_val.map(format_dataset_chatml)\n",
    "\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.add_tokens([\"<HOLDING>\"])\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_double_quant = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_double_quant,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n",
    "          attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "#model2 = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map).base_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "class CustomEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=5, threshold=0.0):\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.best_f1 = 0\n",
    "        self.patience_counter = 0\n",
    "        self.rouge = evaluate.load(\"rouge\") \n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.patience_counter = 0\n",
    "        self.best_f1 = 0\n",
    "\n",
    "    def get_answer(self, predict, label):\n",
    "        answer = predict.strip().split(\"<|assistant|>\")[-1].strip().replace(\"<|end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "        label_val = label.strip().split(\"<|assistant|>\")[-1].strip().replace(\"<|end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "        if answer[0].isdigit():\n",
    "            return int(answer[0]), int(label_val[0])\n",
    "        else:\n",
    "            label_txt = label.strip().split(\"<|assistant|>\")[0].strip().split(f\"{label_val[0]}.\")[1].strip().split(\"\\n\")[0].strip()\n",
    "            scores = self.rouge.compute(predictions=[answer], references=[label_txt])\n",
    "            if sum(scores.values())==4:\n",
    "                return int(label_val[0]), int(label_val[0])\n",
    "            else:\n",
    "                return -1, int(label_val[0])\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        eval_dataloader = kwargs.get('eval_dataloader')\n",
    "        model = kwargs.get('model')\n",
    "\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            inputs = batch['input_ids'].to(args.device)\n",
    "            labels = batch['labels'].to(args.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predictions = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), )\n",
    "                labels = tokenizer.batch_decode(labels)\n",
    "            for prediction, label in zip(predictions, labels):\n",
    "                pred, refs = self.get_answer(prediction, label)\n",
    "                all_predictions.append(pred)\n",
    "                all_labels.append(refs)\n",
    "        f1 = f1_score(y_true=all_labels, y_pred=all_predictions, average='micro')\n",
    "        state.eval_loss = f1\n",
    "\n",
    "        print(f\"Eval f1: {f1}\")\n",
    "\n",
    "        if state.epoch > 0: # at least 1 epoch\n",
    "            if f1 >= self.best_f1 - self.threshold:\n",
    "                self.best_f1 = f1\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "\n",
    "            if self.patience_counter >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Early stopping triggered with best f1 is {self.best_f1}!\")\n",
    "                print(f\"Total steps: {state.global_step}\")\n",
    "        return control\n",
    "\n",
    "early_stopping = CustomEarlyStoppingCallback(patience=patience)\n",
    "\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def _save_checkpoint(self, model, trial=None, metrics=None):\n",
    "        # Copy from Trainer but remove 2 lines to avoid eval_ prefix\n",
    "        checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n",
    "        if self.hp_search_backend is None and trial is None:\n",
    "            self.store_flos()\n",
    "        run_dir = self._get_output_dir(trial=trial)\n",
    "        output_dir = os.path.join(run_dir, checkpoint_folder)\n",
    "        self.save_model(output_dir, _internal_call=True)\n",
    "        if not self.args.save_only_model:\n",
    "            self._save_optimizer_and_scheduler(output_dir)\n",
    "            self._save_rng_state(output_dir)\n",
    "        if metrics is not None and self.args.metric_for_best_model is not None:\n",
    "            metric_to_check = self.args.metric_for_best_model\n",
    "            if metric_to_check in metrics:\n",
    "                metric_value = metrics[metric_to_check]\n",
    "                operator = np.greater if self.args.greater_is_better else np.less\n",
    "                if (\n",
    "                    self.state.best_metric is None\n",
    "                    or self.state.best_model_checkpoint is None\n",
    "                    or operator(metric_value, self.state.best_metric)\n",
    "                ):\n",
    "                    self.state.best_metric = metric_value\n",
    "                    self.state.best_model_checkpoint = output_dir\n",
    "        if self.args.should_save:\n",
    "            self.state.stateful_callbacks[\"TrainerControl\"] = self.control.state()\n",
    "            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n",
    "        if self.args.push_to_hub:\n",
    "            self._push_from_checkpoint(output_dir)\n",
    "        if self.args.should_save:\n",
    "            self._rotate_checkpoints(use_mtime=False, output_dir=run_dir)\n",
    "\n",
    "run_name = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=local_model_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    do_eval=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    log_level=\"debug\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    eval_steps=evalsteps,\n",
    "    logging_steps=logsteps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=None,\n",
    "    seed=342,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    run_name=run_name,\n",
    "    save_steps=save_step,\n",
    "    # prediction_loss_only=False, # to enable computing metrics\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    " \n",
    "trainer = CustomSFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_chatml,\n",
    "    eval_dataset=dataset_val,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "if isSetupParams==False:\n",
    "    trainer.save_model(local_model_dir)\n",
    "\n",
    "    # trainer.push_to_hub(hf_model_repo)\n",
    "\n",
    "    tokenizer.save_pretrained(local_model_dir)\n",
    "    model.save_pretrained(local_model_dir)\n",
    "\n",
    "    trainer.push_to_hub(hf_model_repo)\n",
    "    tokenizer.push_to_hub(hf_model_repo)\n",
    "    model.push_to_hub(hf_model_repo)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
