{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhk/miniconda3/envs/icta2024/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "EVALUATE ./CH-Phi3m4k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhk/miniconda3/envs/icta2024/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.50s/it]\n",
      "Evaluating:   0%|          | 0/3600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Evaluating:   0%|          | 10/3600 [00:05<26:55,  2.22it/s] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating: 100%|██████████| 3600/3600 [25:55<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete\n",
      "F1 Score: 76.89\n",
      "\n",
      "\n",
      "\n",
      "EVALUATE microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n",
      "Evaluating:   0%|          | 0/3600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Evaluating: 100%|██████████| 3600/3600 [1:19:59<00:00,  1.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete\n",
      "F1 Score: 64.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, re, gc, evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "# Bits and Bytes configuration for the model\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_double_quant = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_double_quant,\n",
    ")\n",
    "\n",
    "# Load Model on GPU \n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Load the evaluation dataset\n",
    "eval_dataset = load_dataset(\"lex_glue\", \"case_hold\", split=\"test\")\n",
    "# eval_dataset = eval_dataset.take(100)\n",
    "\n",
    "# Prepare the evaluation data without including the label in the input text\n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(row['endings'])])\n",
    "    user = {\n",
    "        \"content\": f\"Choose the best option to fill <HOLDING> based on this context, answer only with the number of option(for example: 0):\\n{row['context']}\\nOptions to choose:\\n{answers}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    return {\"messages\": messages, \"label\": row[\"label\"]}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False), \"label\": row[\"label\"]}\n",
    "\n",
    "\n",
    "def format_func(example):\n",
    "    answers = \"\".join([f\"{i}. {val}\\n\" for i, val in enumerate(example['endings'])])\n",
    "    text = f\"<|user|>Choose the best option to fill <HOLDING> based on this context, answer only with the number of option (for example: 0):\\n{example['context']}\\nOptions to choose:\\n{answers}<|end|>\\n<|assistant|>\"\n",
    "    return {\"text\": text, \"label\": example[\"label\"], \"endings\": example[\"endings\"], \"context\":  example[\"context\"]}\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "models = [\"./CH-Phi3m4k\",\"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "\n",
    "for model in models:\n",
    "    local_model_dir = model\n",
    "    print(f\"\\n\\n\\nEVALUATE {local_model_dir}\")\n",
    "    try:\n",
    "        # Load the tokenizer and model from the local directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_dir, trust_remote_code=True, device_map=device_map, attn_implementation=attn_implementation)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_model_dir, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map, attn_implementation=attn_implementation)\n",
    "\n",
    "        eval_dataset = eval_dataset.map(format_func)\n",
    "        # eval_dataset = eval_dataset.map(format_dataset_chatml)\n",
    "\n",
    "        # Set up the evaluation pipeline\n",
    "        evaluation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "        # Perform evaluation\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        error_answers = []\n",
    "\n",
    "        for i, example in enumerate(tqdm(eval_dataset, desc=\"Evaluating\")):\n",
    "            input_text = example['text']\n",
    "            expected_label = example['label']\n",
    "\n",
    "            expected_answer = [str(example['label']), example['endings'][example['label']], str(example['label']) +\". \"+ example['endings'][example['label']]]\n",
    "            # expected_answer = \"\\n\".join(expected_answer_arr)\n",
    "\n",
    "            result = evaluation_pipeline(input_text, max_length=1024, num_return_sequences=1)\n",
    "            generated_text = result[0]['generated_text']\n",
    "\n",
    "            # print(f\"Expected Label: {expected_label}\")\n",
    "            # print(f\"Generated Text: {generated_text}\")\n",
    "\n",
    "            try:\n",
    "                answer = generated_text.strip().split(\"<|assistant|>\")[-1].strip().replace(\"<|end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "                predicted_label = int(re.search(r\"(\\d+)\", answer).group(1))\n",
    "                if(predicted_label != expected_label):\n",
    "                    pred = [generated_text]\n",
    "                    refs = [expected_answer]\n",
    "                    scores = rouge.compute(predictions=pred, references=refs)\n",
    "                    total_score = sum(scores.values())\n",
    "                    if(total_score == 4):\n",
    "                        predicted_label = expected_label\n",
    "                    else:\n",
    "                        predicted_label = -1\n",
    "            except Exception:\n",
    "                    predicted_label = -1\n",
    "            \n",
    "            true_labels.append(expected_label)\n",
    "            predicted_labels.append(predicted_label)\n",
    "            if(predicted_label == -1):\n",
    "                error_answers.append({\"label\": expected_label, \"response\": generated_text})\n",
    "\n",
    "        print(\"Evaluation Complete\")\n",
    "        # Calculate micro and macro F1 scores\n",
    "        micro_f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
    "        # macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        print(f\"F1 Score: {(micro_f1*100):.2f}\")\n",
    "        # print(f\"Macro F1 Score: {(macro_f1*100):.2f}\")\n",
    "\n",
    "        # Save the error answers to json file\n",
    "        err_file_name = local_model_dir.replace(\"/\", \"_\")\n",
    "        error_file = f\"{err_file_name}_error_answers.json\"\n",
    "        with open(error_file, \"w\") as f:\n",
    "            f.write(str(error_answers))\n",
    "\n",
    "        # Clear memory after evaluation\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        # clear RAM after finishing the evaluation\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icta2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
